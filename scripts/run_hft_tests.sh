#!/bin/bash

# é«˜é¢‘äº¤æ˜“ç³»ç»Ÿå®Œæ•´æµ‹è¯•æ‰§è¡Œè„šæœ¬
# åŒ…æ‹¬æ€§èƒ½æµ‹è¯•ã€åŠŸèƒ½æµ‹è¯•ã€é›†æˆæµ‹è¯•å’Œå¯é æ€§æµ‹è¯•

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$PROJECT_ROOT"

# åˆ›å»ºæ—¥å¿—ç›®å½•
LOGS_DIR="logs/tests"
mkdir -p "$LOGS_DIR"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
TEST_LOG="$LOGS_DIR/hft_tests_$TIMESTAMP.log"

log_info "é«˜é¢‘äº¤æ˜“ç³»ç»Ÿå®Œæ•´æµ‹è¯•å¼€å§‹..."
log_info "æµ‹è¯•æ—¥å¿—å°†ä¿å­˜åˆ°: $TEST_LOG"

# æ£€æŸ¥Pythonç¯å¢ƒ
log_info "æ£€æŸ¥Pythonç¯å¢ƒ..."
if ! command -v python &> /dev/null; then
    log_error "Pythonæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
    exit 1
fi

PYTHON_VERSION=$(python --version 2>&1)
log_info "Pythonç‰ˆæœ¬: $PYTHON_VERSION"

# æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
if [[ -d ".venv" ]]; then
    log_info "æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ..."
    source .venv/bin/activate
else
    log_warning "æœªæ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒï¼Œå°†ä½¿ç”¨ç³»ç»ŸPython"
fi

# å®‰è£…æµ‹è¯•ä¾èµ–
log_info "å®‰è£…æµ‹è¯•ä¾èµ–..."
if [[ -f "pyproject.toml" ]]; then
    uv pip install -e ".[test]" >> "$TEST_LOG" 2>&1 || {
        log_warning "ä½¿ç”¨uvå®‰è£…å¤±è´¥ï¼Œå°è¯•pip..."
        pip install -e ".[test]" >> "$TEST_LOG" 2>&1
    }
else
    pip install pytest pytest-asyncio pytest-cov psutil >> "$TEST_LOG" 2>&1
fi

# æ£€æŸ¥å¿…è¦çš„æ¨¡å—æ˜¯å¦å¯å¯¼å…¥
log_info "æ£€æŸ¥æ¨¡å—å¯¼å…¥..."
python -c "
try:
    import pytest
    import asyncio 
    import psutil
    print('âœ… æ‰€æœ‰å¿…è¦æ¨¡å—éƒ½å¯ä»¥å¯¼å…¥')
except ImportError as e:
    print(f'âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}')
    exit(1)
" || {
    log_error "æ¨¡å—å¯¼å…¥æ£€æŸ¥å¤±è´¥"
    exit 1
}

# æµ‹è¯•å‡½æ•°
run_test_suite() {
    local test_name="$1"
    local test_path="$2" 
    local extra_args="$3"
    
    log_info "è¿è¡Œ $test_name..."
    
    local test_output_file="$LOGS_DIR/${test_name}_$TIMESTAMP.log"
    
    if pytest "$test_path" $extra_args -v --tb=short > "$test_output_file" 2>&1; then
        log_success "$test_name é€šè¿‡"
        # æ˜¾ç¤ºç®€è¦ç»“æœ
        tail -n 10 "$test_output_file" | grep -E "(PASSED|FAILED|ERROR|===)"
    else
        log_error "$test_name å¤±è´¥"
        log_error "è¯¦ç»†é”™è¯¯ä¿¡æ¯è¯·æŸ¥çœ‹: $test_output_file"
        # æ˜¾ç¤ºé”™è¯¯æ‘˜è¦
        tail -n 20 "$test_output_file" | grep -E "(FAILED|ERROR|AssertionError)"
        return 1
    fi
}

# æµ‹è¯•æ‰§è¡Œè®¡åˆ’
declare -a test_suites=(
    "æ€§èƒ½æµ‹è¯•:tests/test_hft_system_validation.py::TestSystemPerformance:-x"
    "åŠŸèƒ½æµ‹è¯•:tests/test_hft_system_validation.py::TestFunctionalValidation:-x"
    "é›†æˆæµ‹è¯•:tests/test_hft_system_validation.py::TestIntegrationValidation:-x"
    "å¯é æ€§æµ‹è¯•:tests/test_hft_system_validation.py::TestReliabilityValidation:-x"
    "åŸºå‡†æµ‹è¯•:tests/test_hft_system_validation.py::TestSystemBenchmarks:-s --tb=short"
)

# æ‰§è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶
failed_tests=0
total_tests=${#test_suites[@]}

log_info "å¼€å§‹æ‰§è¡Œ $total_tests ä¸ªæµ‹è¯•å¥—ä»¶..."
echo "=============================================="

for test_suite in "${test_suites[@]}"; do
    IFS=':' read -r test_name test_path extra_args <<< "$test_suite"
    
    if ! run_test_suite "$test_name" "$test_path" "$extra_args"; then
        ((failed_tests++))
    fi
    
    echo "----------------------------------------------"
done

# è¿è¡Œè¦†ç›–ç‡æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
if [[ "$1" == "--coverage" ]]; then
    log_info "è¿è¡Œæµ‹è¯•è¦†ç›–ç‡åˆ†æ..."
    
    coverage_file="$LOGS_DIR/coverage_$TIMESTAMP.html"
    
    pytest tests/test_hft_system_validation.py \
        --cov=src/hft \
        --cov-report=html:"$coverage_file" \
        --cov-report=term \
        >> "$TEST_LOG" 2>&1
    
    if [[ $? -eq 0 ]]; then
        log_success "è¦†ç›–ç‡æŠ¥å‘Šç”ŸæˆæˆåŠŸ: $coverage_file"
    else
        log_warning "è¦†ç›–ç‡åˆ†æå¤±è´¥"
    fi
fi

# ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
log_info "ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š..."

cat << EOF > "$LOGS_DIR/test_report_$TIMESTAMP.md"
# é«˜é¢‘äº¤æ˜“ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚å†µ
- æµ‹è¯•æ—¶é—´: $(date)
- Pythonç‰ˆæœ¬: $PYTHON_VERSION
- æµ‹è¯•å¥—ä»¶æ€»æ•°: $total_tests
- é€šè¿‡æ•°é‡: $((total_tests - failed_tests))
- å¤±è´¥æ•°é‡: $failed_tests

## æµ‹è¯•ç»“æœ
EOF

for test_suite in "${test_suites[@]}"; do
    IFS=':' read -r test_name test_path extra_args <<< "$test_suite"
    test_result_file="$LOGS_DIR/${test_name}_$TIMESTAMP.log"
    
    if [[ -f "$test_result_file" ]]; then
        echo "### $test_name" >> "$LOGS_DIR/test_report_$TIMESTAMP.md"
        
        # æå–å…³é”®ä¿¡æ¯
        if grep -q "FAILED" "$test_result_file"; then
            echo "âŒ çŠ¶æ€: å¤±è´¥" >> "$LOGS_DIR/test_report_$TIMESTAMP.md"
        else
            echo "âœ… çŠ¶æ€: é€šè¿‡" >> "$LOGS_DIR/test_report_$TIMESTAMP.md"
        fi
        
        # æå–æ€§èƒ½æ•°æ®
        grep -E "(å»¶è¿Ÿ|åå|å†…å­˜|æˆåŠŸç‡)" "$test_result_file" | head -5 >> "$LOGS_DIR/test_report_$TIMESTAMP.md"
        echo "" >> "$LOGS_DIR/test_report_$TIMESTAMP.md"
    fi
done

# æµ‹è¯•ç»“æœæ±‡æ€»
echo "=============================================="
log_info "æµ‹è¯•æ‰§è¡Œå®Œæ¯•"
log_info "æµ‹è¯•æŠ¥å‘Š: $LOGS_DIR/test_report_$TIMESTAMP.md"

if [[ $failed_tests -eq 0 ]]; then
    log_success "æ‰€æœ‰æµ‹è¯•å¥—ä»¶é€šè¿‡! ğŸ‰"
    
    # è¿è¡Œç³»ç»ŸåŸºå‡†æµ‹è¯•
    log_info "è¿è¡Œç³»ç»ŸåŸºå‡†è¯„ä¼°..."
    python -c "
import asyncio
import sys
sys.path.append('.')
from tests.test_hft_system_validation import TestSystemBenchmarks

async def run_benchmark():
    benchmark = TestSystemBenchmarks()
    await benchmark.test_system_benchmark_suite()

if __name__ == '__main__':
    asyncio.run(run_benchmark())
" >> "$TEST_LOG" 2>&1
    
    echo ""
    log_success "ç³»ç»Ÿå·²é€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼Œå¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ! ğŸš€"
    exit 0
else
    log_error "æœ‰ $failed_tests ä¸ªæµ‹è¯•å¥—ä»¶å¤±è´¥"
    log_error "è¯·æ£€æŸ¥æµ‹è¯•æ—¥å¿—å¹¶ä¿®å¤é—®é¢˜åé‡è¯•"
    exit 1
fi